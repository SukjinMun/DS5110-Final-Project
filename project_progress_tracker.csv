Task,Description,Assigned,Plan Start,Plan End,Act Start,Act End,Status,Notes
1,"Form team and assign roles: Establish Team 22 with three members and define clear role boundaries. Suk Jin assigned as backend API developer responsible for statistical modeling, business logic, and Flask API design. Xiaobai assigned as frontend developer responsible for data visualization, dashboards, and optional web UI. Shaobo assigned as database architect responsible for schema design, data generation, ETL pipelines, and SQL query development.",All,Sep Week 1,Sep Week 2,Sep Week 1,Sep Week 2,Done,Suk Jin (backend API developer) Xiaobai (frontend developer) Shaobo (database architect)
2,Select project topic: Initial selection of hospital database project focusing on general operations. Team selected healthcare domain due to data availability and real-world relevance. Scope includes multiple hospital departments and operational metrics.,All,Sep Week 1,Sep Week 3,Sep Week 1,Sep Week 3,Done,Hospital database for operations
3,"Submit iteration 01: Compile and submit initial project proposal including team composition, preliminary topic selection, and high-level project objectives. Package deliverables and submit via Canvas LMS platform.",Suk Jin,Sep Week 3,Sep Week 4,Sep Week 3,Sep Week 4,Done,Submitted on Canvas
4,"Review course materials: Comprehensive review of DS5110 course content from weeks 1-6 covering database fundamentals, SQL, data importing, cleaning, transformation, and visualization. Read textbook Chapters 3 (Linear Regression) and 4 (Classification, GLMs, Poisson Regression) to understand statistical modeling requirements for project.",All,Oct Week 1,Oct Week 3,Oct Week 1,Oct Week 4,Done,Weeks 1-6 lectures and reading Ch3-Ch4
5,"Review professor feedback: Analyze iteration 01 feedback indicating project scope was too broad without specific analytical purpose. Identify key concern that project must integrate database design with statistical analysis, not just general operations tracking. Understand requirement to apply all course concepts cohesively.",All,Oct Week 2,Oct Week 2,Oct Week 2,Oct Week 2,Done,Topic too broad - need specific focus
6,Refine project scope: Narrow focus from general hospital operations to specific Emergency Department (ED) triage classification and patient flow analysis. Define three specific analytical questions: (1) Can we predict patient urgency from initial assessment? (2) What factors drive ED wait times? (3) How can we forecast daily visit volume for staffing? Establish measurable outcomes and clear connection to course learning objectives.,All,Oct Week 2,Oct Week 3,Oct Week 2,Oct Week 3,Done,ED triage classification and wait time
7,"Design database schema: Create initial conceptual schema for ED database including core entities (patients, visits, triage, staff, departments, treatments, wait_times). Design entity-relationship diagram showing cardinalities and relationships. Apply normalization principles to achieve at least 3NF. Define primary keys, foreign keys, and candidate keys for all entities.",Shaobo,Oct Week 3,Oct Week 4,Oct Week 3,Oct Week 4,Done,Initial schema design in 3NF
8,"Write 3-page report: Compose comprehensive Overleaf LaTeX document addressing all iteration 02 requirements: (1) Project kickoff section covering goals, scope, deliverables, milestones, team capabilities, and dataset strategy. (2) Team discussions section detailing skills, expertise mapping, challenges, tools, and language selection. (3) Skills assessment section covering external resources, frameworks, proficiency plans, and task assignments. (4) Initial setup section documenting development environment and version control configuration. Include submission verification confirming all requirements met.",All,Oct Week 3,Oct Week 4,Oct Week 3,Oct Week 4,Done,Addresses all iteration 02 requirements
9,"Create progress tracker: Design and implement CSV-based project progress tracker with fields for task number, description, assigned team member, planned start/end dates, actual start/end dates, status, and notes. Populate with all tasks across iterations 02-05. Establish tracking methodology for monitoring project progress throughout semester.",Suk Jin,Oct Week 4,Oct Week 4,Oct Week 4,Oct Week 4,Done,This file
10,"Setup GitHub repo: Initialize GitHub repository at https://github.com/SukjinMun/DS5110-Final-Project. Configure repository structure with three-tier architecture directories: db/ for database files, backend/ for Flask API code, frontend/ for web interface, plus directories for scripts, notebooks, and reports. Establish feature branch workflow where each member develops under dedicated branches (dev-db for Shaobo, dev-api for Suk Jin, dev-ui for Xiaobai) and merges weekly into main branch. Create comprehensive README.md documenting project overview, objectives, team roles, and setup instructions. Configure .gitignore for Python projects. Grant write access to all team members (add Shaobo as GideonCCC and Xiaobai as leeshirley2002@gmail.com).",Suk Jin,Oct Week 4,Oct Week 4,Oct Week 4,Oct Week 4,Done,Public repo with README and CSV
11,"Submit iteration 02: Package all iteration 02 deliverables including 3-page PDF report generated from Overleaf LaTeX, progress tracker CSV file, and GitHub repository link. Verify all requirements addressed: project introduction, data source link (simulated data methodology), progress tracking confirmation, and submission readiness verification. Submit via Canvas LMS platform before deadline.",Suk Jin,Oct Week 4,Oct Week 4,Oct Week 4,Oct Week 4,Done,Submitted on Canvas
12,"Design and implement SQLite schema: Transform conceptual schema into physical implementation using SQLite. Create SQL DDL statements defining all tables with appropriate data types (INTEGER, TEXT, REAL, DATETIME). Implement constraints including PRIMARY KEY, FOREIGN KEY with ON DELETE and ON UPDATE rules, UNIQUE constraints, CHECK constraints for data validation (e.g., urgency_level BETWEEN 1 AND 5, heart_rate BETWEEN 40 AND 200). Create indexes on frequently queried columns to optimize performance. Generate detailed ERD using tools like draw.io or dbdiagram.io showing all tables, columns, data types, and relationships. Ensure schema achieves 3NF with no transitive dependencies or partial key dependencies. Deliverables: schema.sql file (Shaobo), ERD diagram PNG or PDF (Shaobo), data dictionary documenting each table and column with descriptions and constraints (Shaobo).",Shaobo,Iter3 Week 1,Iter3 Week 2,,,,Normalized schema with constraints and ERD
13,"Build data generation script: Develop Python script using NumPy and Pandas to generate realistic simulated ED data (5000-10000 patient visit records). Use distributions from CDC National Hospital Ambulatory Medical Care Survey and data.gov sources. Generate patient demographics (age using appropriate age distribution skewed towards elderly, gender 52% female/48% male, insurance status). Simulate arrival times following realistic patterns with peaks during evening hours (5-9 PM) and weekends using Poisson distribution. Generate vital signs correlated with urgency levels: heart rate (40-200 bpm), blood pressure (systolic 80-200, diastolic 40-120), temperature (95-105°F), oxygen saturation (85-100%). Assign urgency levels (ESI 1-5) with appropriate distribution. Calculate wait times based on urgency, time of day, and patient volume using realistic formulas. Ensure referential integrity with proper foreign key relationships. Include random seed for reproducibility. Deliverables: data_generation.py script (Shaobo), requirements.txt for dependencies (Shaobo), documentation of distributions used (Shaobo).",Shaobo,Iter3 Week 1,Iter3 Week 2,,,,Simulated ED patient visits with https://data.gov/ distributions
14,"Populate database: Develop ETL pipeline script to load generated CSV data into SQLite database. Create Python script using sqlite3 or SQLAlchemy to establish database connection, create tables using schema.sql, and load data from CSV files into appropriate tables. Implement transaction management with rollback capability for error handling. Add data validation checks during loading (check for NULL values in NOT NULL columns, validate foreign key references exist, verify CHECK constraints). Log loading process with row counts for each table. Include data verification queries to confirm successful loading (count records, check for orphaned foreign keys, validate date ranges). Deliverables: etl_pipeline.py script (Shaobo), loading log file (Shaobo), verification queries output (Shaobo).",Shaobo,Iter3 Week 2,Iter3 Week 3,,,,Load generated data into SQLite via ETL pipeline
15,"Validate data integrity: Perform comprehensive data integrity validation on populated database. Write and execute SQL queries to verify: (1) No orphaned foreign key records using LEFT JOIN queries. (2) All CHECK constraints properly enforced (test by attempting invalid insertions). (3) UNIQUE constraints working (verify no duplicate records where uniqueness required). (4) NOT NULL constraints enforced on required fields. (5) Referential integrity maintained (CASCADE operations work correctly). (6) Data ranges realistic (heart rate 40-200, age 0-120, urgency 1-5). (7) Date/time consistency (departure_time > arrival_time, timestamps within expected year). Create validation report documenting all checks performed and results. Fix any integrity violations discovered. Deliverables: validation_queries.sql file (Shaobo), validation report in PDF or markdown format (Shaobo), corrected database if issues found (Shaobo).",Shaobo,Iter3 Week 2,Iter3 Week 3,,,,Foreign keys and referential constraints
16,"Document ETL pipeline: Create comprehensive documentation for entire ETL process from data generation through database loading. Document includes: (1) Data generation methodology section explaining distributions used, sources cited (CDC, data.gov), parameters chosen, and justification for realistic simulation. (2) Database schema section with ERD diagram, table descriptions, column definitions with data types and constraints, relationship explanations. (3) ETL process section with flowchart showing steps from CSV generation to database loading, error handling procedures, validation checkpoints. (4) Usage instructions with prerequisites, step-by-step execution commands, expected outputs, troubleshooting common issues. (5) Data dictionary with every table and column documented. Format as Markdown or PDF. Deliverables: ETL_documentation.md or PDF file (Shaobo), flowchart diagram (Shaobo).",Shaobo,Iter3 Week 2,Iter3 Week 3,,,,Data generation and loading process
16.5,"Create ETL process flowcharts and diagrams: Design visual flowcharts showing ETL pipeline steps from data generation through database loading. Create process diagrams illustrating data flow, transformation steps, and validation checkpoints. Support Shaobo documentation with professional diagrams. Deliverables: ETL flowchart diagrams (Xiaobai), process flow visualizations (Xiaobai).",Xiaobai,Iter3 Week 2,Iter3 Week 3,,,,ETL visualization support
17,"Develop SQL analytical queries: Write at least 8 complex SQL queries demonstrating database analytical capabilities for ED operations insights. Required queries: (1) Patient counts grouped by urgency level with percentages - use GROUP BY and COUNT. (2) Average wait times by urgency level and time of day - use GROUP BY, AVG, CASE for time binning. (3) Staff workload distribution showing patients per staff member by shift - use JOIN, GROUP BY, COUNT. (4) Visit trends over time showing daily/weekly patterns - use DATE functions, GROUP BY with temporal aggregation. (5) Repeat patients within 30 days - use self-join or window functions with date arithmetic. (6) Case ratios showing distribution of chief complaints - use GROUP BY, COUNT, percentage calculations. (7) Resource utilization showing bed/room usage over time - use aggregations with temporal joins. (8) Arrival/completion time-series for identifying bottlenecks - use window functions and moving averages. Each query should include comments explaining business logic. Deliverables: analytical_queries.sql file with commented queries (Shaobo), query_results.md showing sample outputs (Shaobo).",Shaobo,Iter3 Week 2,Iter3 Week 3,,,,At least 8 analytical queries: patient counts by urgency average wait times staff workload visit trends repeat patients case ratios resource utilization time-series
17.6,"Document SQL query results and create summary visualizations: Execute Shaobo analytical SQL queries and document results. Create summary tables and visualizations for each query showing key insights. Prepare preliminary analytics that will inform later dashboard design. Deliverables: SQL query results documentation (Xiaobai), query output visualizations (Xiaobai).",Xiaobai,Iter3 Week 2,Iter3 Week 3,,,,SQL query visualization support
17.7,"Review and approve database work for GitHub: Review Shaobo completed database schema, data generation scripts, ETL pipeline, and SQL queries. Verify code quality and documentation completeness. As repository admin/owner, approve the work and commit/push to GitHub repository with descriptive commit messages. Ensure proper directory structure and file organization. Note: teammates require admin approval before their work can be pushed to the repository. Deliverables: GitHub commits for database work (Suk Jin approval required), updated repository (Suk Jin).",Suk Jin,Iter3 Week 2,Iter3 Week 3,,,,Git repository management - admin approval required
17.5,"Study course materials for EDA and visualization: Review lecture materials and in-class work examples related to exploratory data analysis and visualization techniques. Study InClassWork_02 materials including PandasReference.ipynb and Seaborn_tutorial.ipynb. Review lecture materials from week4 including PandasReference.ipynb, Part_01.ipynb, Part_02.ipynb, and Seaborn_tutorial.ipynb. Practice creating different types of plots using sample datasets. Understand best practices for data visualization and statistical summaries. This preparation will ensure high quality EDA and visualizations for the project.",Xiaobai,Iter3 Week 1,Iter3 Week 2,,,,Study materials for EDA work
18.5,"Create data quality visualizations: Develop visualizations showing data quality metrics from validation results. Create charts for missing value patterns, outlier detection plots, distribution comparisons, and constraint violation summaries. Support Shaobo validation work with visual reporting. Deliverables: data quality notebook (Xiaobai), validation visualization figures (Xiaobai).",Xiaobai,Iter3 Week 2,Iter3 Week 3,,,,Data validation visualization support
18,"Exploratory data analysis: Perform initial EDA on generated ED data using Pandas and Jupyter Notebook. Calculate summary statistics for all numerical variables including mean, median, standard deviation, quartiles, minimum and maximum values using the describe function. Analyze distributions for demographics such as age, gender, and insurance status. Examine vital signs including heart rate, blood pressure, temperature, and oxygen saturation. Review urgency levels, wait times, and visit patterns. Identify potential outliers using interquartile range method or z-scores. Check for missing values and data quality issues. Analyze correlations between variables using correlation matrices. Examine temporal patterns such as arrival times by hour, day, and month. Document findings with statistical tables. This analysis informs subsequent modeling and visualization work. Deliverables: EDA.ipynb Jupyter notebook with analysis code and narrative (Xiaobai), summary_statistics.csv file (Xiaobai), findings summary document (Xiaobai).",Xiaobai,Iter3 Week 3,Iter3 Week 3,,,,Summary stats and distributions
19,"Create initial visualizations: Develop exploratory visualizations using Matplotlib, Seaborn, and Plotly to understand ED data patterns. Create time series plots showing daily visit volumes, hourly arrival patterns, and trends over time with moving averages. Develop distribution plots including histograms of age, wait times, and vital signs. Create box plots of wait times by urgency level. Design violin plots showing distributions by categorical variables. Generate categorical plots with bar charts of urgency level frequencies, chief complaint distributions, and insurance status breakdown. Build correlation heatmaps showing relationships between vital signs, wait times, urgency levels, and other metrics. Produce scatter plots examining relationships like staff count versus wait time, patient volume versus wait time, and age versus urgency level. Use appropriate color schemes, labels, titles, and legends. Save high-resolution figures for reporting. Deliverables: visualizations.ipynb notebook generating all plots (Xiaobai), figures directory with saved PNG and PDF visualizations (Xiaobai).",Xiaobai,Iter3 Week 3,Iter3 Week 3,,,,Time series and correlation plots
19.5,"Study course materials for statistical modeling: Review reading assignments and lecture materials related to classification, regression, and Poisson models. Complete thorough reading of Ch3_Linear_Regression.pdf from ReadingAssignment02 folder taking notes on key concepts. Read all six parts of Ch4_Classification from ReadingAssignment02 folder covering logistic regression, Linear Discriminant Analysis, Naive Bayes, and Generalized Linear Models including Poisson regression. Review study notes already created in studynotes_ch3.txt and studynotes_ch4 files. Study lecture materials from week5 including Lec_06_01.ipynb and Lec_06_02.ipynb which cover model training. Review week6 materials including AllModels.ipynb and Bias_Variance_Lecture.ipynb. Review Train_Testpynb.ipynb to understand train-test split methodology. Document key concepts, formulas, and implementation approaches. This reading provides theoretical foundation for modeling tasks in iteration 04.",Suk Jin,Iter3 Week 1,Iter3 Week 2,,,,Reading and studying modeling theory
19.6,"Complete InClassWork_04 modeling exercises: Work through InClassWork_04 which directly practices skills needed for project modeling tasks. Complete partA_eda.py to practice exploratory data analysis on a dataset. Work through partB_preprocessing.py to learn data preprocessing and pipeline creation. Execute partB_verify_pipeline.py to understand pipeline validation. Complete partC_train_models.py to practice training multiple classification or regression models. Work through partD_diagnostics.py to learn model diagnostic techniques. Complete partE_predictions.py to practice making predictions with trained models. Use compare_diagnostics.py to understand model comparison approaches. Reference the Lec_06_01.ipynb and Lec_06_02.ipynb materials in the references folder as needed. Document the code, results, and lessons learned from each part. This hands-on practice directly prepares for Tasks 22 through 26. Deliverables: completed Python scripts for all parts with comments (Suk Jin), results output files (Suk Jin), reflection document on lessons learned and how techniques apply to ED project (Suk Jin).",Suk Jin,Iter3 Week 2,Iter3 Week 3,,,,Hands-on modeling practice
19.7,"Develop model prototypes with sample data: Create actual working prototypes of the three model types needed for iteration 04 using publicly available sample datasets. Build a classification model prototype using scikit-learn with a multi-class dataset such as iris or wine dataset. Implement logistic regression, Linear Discriminant Analysis, and Naive Bayes models. Create train-test split, perform hyperparameter tuning using GridSearchCV, generate confusion matrices, and calculate performance metrics. Develop a regression model prototype using a continuous outcome dataset. Implement simple and multiple linear regression. Perform residual diagnostics including residual plots and QQ plots. Check for multicollinearity using VIF. Build a Poisson regression prototype using statsmodels with a count data example. Fit GLM with Poisson family, check for overdispersion, interpret exponentiated coefficients. Save all prototype code as templates that can be adapted when ED data becomes available. Test that all required libraries work correctly in your environment. Deliverables: classification_prototype.py with three models (Suk Jin), regression_prototype.py with diagnostics (Suk Jin), poisson_prototype.py with GLM (Suk Jin), test results showing all code executes successfully (Suk Jin), documentation of how to adapt prototypes for ED data (Suk Jin).",Suk Jin,Iter3 Week 2,Iter3 Week 3,,,,Create working model prototypes
19.8,"Design and document modeling approach: Create detailed design documents specifying the exact approach for each modeling task in iteration 04. Write a classification model specification document including the list of features to extract from database such as age, gender, vital signs, chief complaint category, and time of arrival. Define target variable encoding for urgency levels 1 through 5. Specify train-validation-test split ratios and stratification approach. List hyperparameters to tune for each model type. Define evaluation metrics including accuracy, precision, recall, F1-score, confusion matrix, and ROC curves. Write a regression model specification document for wait time prediction. List predictor variables, interaction terms to test, and polynomial terms to consider. Specify diagnostic checks to perform. Write a Poisson regression specification document for daily visit counts. Define how to aggregate data to daily level, list predictor variables, and specify goodness-of-fit tests. Create a business logic specification listing all validation rules and automatic calculations needed. Document statistical indicators to compute with formulas. These specifications will guide implementation in iteration 04. Deliverables: classification_model_specification.md (Suk Jin), regression_model_specification.md (Suk Jin), poisson_model_specification.md (Suk Jin), business_logic_specification.md (Suk Jin), statistical_indicators_specification.md (Suk Jin).",Suk Jin,Iter3 Week 2,Iter3 Week 3,,,,Design modeling specifications
19.62,"Implement business logic module early: Develop business_logic.py module with validation and calculation functions that will be needed for data processing and API operations. Create validation functions: (1) validate_patient_data(age, gender, insurance) checking age 0-120, gender in valid values, insurance codes valid. (2) validate_vital_signs(heart_rate, bp_systolic, bp_diastolic, temperature, o2_sat) ensuring HR 40-200 bpm, BP systolic 80-200, BP diastolic 40-120, temp 95-105°F, O2 sat 85-100%. (3) validate_timestamps(arrival, departure) confirming arrival < departure, no future dates. (4) validate_urgency_level(level) checking level 1-5. Implement calculation functions: (1) calculate_wait_time(triage_time, doctor_time) returning minutes. (2) calculate_length_of_stay(arrival, departure) returning minutes. (3) calculate_age_from_dob(dob, current_date). (4) assign_shift(timestamp) returning day/evening/night based on hour. Include comprehensive docstrings. Write unit tests using pytest for all functions with edge cases. This module can be developed and tested independently, then used in both data processing and API endpoints later. Deliverables: business_logic.py module with all functions (Suk Jin), test_business_logic.py with pytest unit tests (Suk Jin), business_logic_documentation.md (Suk Jin).",Suk Jin,Iter3 Week 2,Iter3 Week 3,,,,Build business logic module - independent development
19.9,"Design backend API architecture and database integration layer: Study Shaobo's completed database schema and ERD to understand table structures, relationships, and data types. Design the backend API architecture that will serve as the integration layer between frontend and database in the three-tier system (Frontend ↔ Backend API ↔ Database). Create API architecture document specifying: (1) RESTful endpoint structure for data retrieval (GET /api/patients, GET /api/visits, GET /api/analytics/*). (2) Prediction endpoints (POST /api/predict/urgency, POST /api/predict/wait-time). (3) Request/response JSON formats for each endpoint. (4) Error handling and HTTP status code conventions. Design database access layer using repository pattern: create specifications for PatientRepository, VisitRepository, and AnalyticsRepository classes that will execute SQL queries and return JSON-serializable results. Document SQL queries that each repository method will execute. Specify how Flask app.py will call repository methods (never direct SQL in routes). Create integration diagram showing request flow: Frontend makes AJAX call → Flask route receives request → Route calls repository method → Repository executes SQL on SQLite database → Results returned as JSON → Frontend displays data. This design work ensures clean separation of concerns and prepares for implementation in iteration 04 (Tasks 29 and 29.5). Dependency: Requires Task 12 (database schema) completion. Deliverables: api_architecture_design.md (Suk Jin), database_access_layer_design.md (Suk Jin), endpoint_specifications.md with request/response examples (Suk Jin), integration_flow_diagram.png or PDF (Suk Jin), sql_queries_by_endpoint.md documenting which queries each endpoint executes (Suk Jin).",Suk Jin,Iter3 Week 3,Iter3 Week 3,,,,Backend API and database integration design - depends on Task 12
19.95,"Review and approve analysis work for GitHub: Review Xiaobai completed EDA notebooks and visualizations, and Suk Jin own business logic and design documents. Verify code quality and documentation completeness. As repository admin/owner, approve all work and commit/push all iteration 03 deliverables to GitHub with organized directory structure and clear commit messages. Update README if needed. Note: teammates require admin approval before their work can be pushed to the repository. Deliverables: GitHub commits for analysis work (Suk Jin approval required), updated repository with all iteration 03 deliverables (Suk Jin).",Suk Jin,Iter3 Week 3,Iter3 Week 3,,,,Git repository management - admin approval required
20,"Mid-project checkpoint meeting (COLLABORATIVE - all team members work together): Conduct comprehensive team meeting to evaluate iteration 03 progress and plan remaining project work. All members actively participate in reviewing completed deliverables including database schema, generated data, ETL pipeline, SQL queries, initial EDA, visualizations, model prototypes, business logic, and API architecture design. Collectively assess quality and completeness against project objectives. Identify any issues or blockers and collaboratively discuss resolution strategies. Confirm roles and task assignments for iteration 04 including statistical modeling and Flask web application development. Update timeline and progress tracker with actual completion dates from iteration 03. Plan coordination strategies for handoffs between team members. Document all decisions and next steps with full team agreement. Ensure all team members understand their upcoming tasks and dependencies. Deliverables: meeting notes document (All), updated project plan (All), updated progress tracker CSV with actual completion dates (Suk Jin).",All,Iter3 Week 3,Iter3 Week 3,,,,Collaborative checkpoint - evaluate progress and plan Flask development
21,"Data cleaning and validation: Perform additional data quality checks and cleaning beyond initial ETL validation. Handle missing values using appropriate strategies: mean/median imputation for numerical variables, mode imputation or separate category for categorical variables, or deletion if missing data < 5%. Validate data ranges for all vital signs: heart rate 40-200 bpm (flag outliers), blood pressure systolic 80-200 and diastolic 40-120 mmHg, temperature 95-105°F, oxygen saturation 85-100%. Remove duplicate records based on composite key logic. Standardize categorical variable formats: consistent capitalization, trimmed whitespace, standardized coding for insurance types and chief complaints. Verify referential integrity constraints hold after any cleaning operations. Document all cleaning decisions and transformations applied with before/after counts. Create cleaned dataset versions for modeling. Deliverables: data_cleaning.py script (Shaobo), cleaning_report.md documenting transformations (Shaobo), cleaned_data.csv files ready for modeling (Shaobo).",Shaobo,Iter4 Week 1,Iter4 Week 1,,,,Handle missing values validate ranges (HR 40-200 temp 95-105) remove duplicates standardize formats enforce referential integrity
22,"Train classification models for triage urgency prediction: Develop and compare multiple classification models to predict patient urgency level (1-5) from initial assessment data. Prepare feature matrix X including: age, gender, vital signs (heart rate, BP systolic/diastolic, temperature, O2 saturation), chief complaint category, time of arrival (hour), previous ED visit count. Target variable y is urgency_level (1-5). Split data into training (70%), validation (15%), and test (15%) sets with stratification by urgency level. Train three models: (1) Multinomial Logistic Regression as baseline using scikit-learn LogisticRegression with multi_class='multinomial'. (2) Linear Discriminant Analysis (LDA) using scikit-learn LinearDiscriminantAnalysis. (3) Naive Bayes using GaussianNB for numerical features. Perform hyperparameter tuning using GridSearchCV with 5-fold cross-validation. Evaluate models using classification metrics: accuracy, precision, recall, F1-score for each urgency class. Create confusion matrices showing misclassification patterns. Analyze feature importance/coefficients to identify key predictors. Document model selection rationale. Deliverables: classification_models.py script (Suk Jin), trained model files in pkl format (Suk Jin), model_comparison.md report (Suk Jin), confusion_matrices.png (Suk Jin).",Suk Jin,Iter4 Week 1,Iter4 Week 2,,,,Logistic regression LDA naive Bayes for urgency prediction
23,"Train regression models for wait time prediction: Build linear regression models to predict continuous wait time outcome (minutes from triage to doctor). Prepare features including: urgency_level (1-5), time_of_day (binned or continuous hour), day_of_week (categorical dummy variables), current_patient_count (number waiting), doctors_on_duty (count), patient_age, interaction terms (urgency × time_of_day). Target variable is wait_time_minutes. Split data 70/15/15 train/validation/test. Start with simple linear regression using single best predictor, then build multiple linear regression with all features. Consider polynomial terms if scatter plots show non-linear relationships. Perform model diagnostics: (1) Residual plots to check homoscedasticity and linearity assumptions. (2) QQ plots to assess normality of residuals. (3) Check for high leverage points and influential observations using Cook's distance. (4) VIF to detect multicollinearity. Compare models using adjusted R², RSE (residual standard error), and cross-validated RMSE. Interpret coefficients in context. Generate prediction intervals for new observations. Deliverables: regression_models.py script (Suk Jin), diagnostic_plots.png (Suk Jin), model_evaluation.md with R² and coefficient interpretations (Suk Jin).",Suk Jin,Iter4 Week 1,Iter4 Week 2,,,,Wait time prediction with diagnostics
24,"Train Poisson regression for daily visit count modeling: Apply Generalized Linear Model (GLM) with Poisson family to model count data - number of ED visits per day. Aggregate data to daily level creating response variable visit_count (0,1,2,...). Create predictor variables: day_of_week (Monday-Sunday as factors), season (Winter/Spring/Summer/Fall), is_holiday (binary 0/1), staff_count (number of staff scheduled), temperature (if weather data available). Use statsmodels GLM with family=Poisson() and log link function. Check for overdispersion by comparing deviance to degrees of freedom - if variance > mean significantly, consider Negative Binomial or Quasi-Poisson alternative. Interpret exponentiated coefficients as multiplicative effects on visit rate. Calculate predicted daily visit counts for resource planning scenarios. Perform residual diagnostics: deviance residuals, Pearson residuals. Conduct goodness-of-fit test. Create visualization comparing observed vs predicted counts. Analyze which factors most influence visit volume for staffing optimization. Deliverables: poisson_regression.py script using statsmodels (Suk Jin), coefficient_interpretation.md (Suk Jin), predicted_vs_actual_plot.png (Suk Jin), staffing_recommendations.md (Suk Jin).",Suk Jin,Iter4 Week 2,Iter4 Week 2,,,,Daily visit count modeling
25,"Implement business logic and validation rules: Develop Python module implementing business logic for ED data validation and automatic calculations. Create validation functions: (1) validate_patient_data() checking age in valid range, required fields present, insurance codes valid. (2) validate_vital_signs() ensuring heart rate, BP, temp, O2 sat within physiologically plausible ranges. (3) validate_timestamps() confirming arrival < departure, dates within expected range, no future dates. (4) validate_referential_integrity() checking all foreign keys reference existing records. Implement automatic calculation functions: (1) calculate_wait_time() computing minutes between triage timestamp and doctor_seen timestamp. (2) calculate_length_of_stay() computing total time from arrival to departure. (3) calculate_age_from_dob() if date of birth provided. (4) assign_shift() determining shift (day/evening/night) from timestamp. Include comprehensive error handling with informative error messages. Write unit tests for all functions. Document business rules applied. Deliverables: business_logic.py module, test_business_logic.py unit tests, business_rules_documentation.md.",Suk Jin,Iter4 Week 1,Iter4 Week 2,,,,Data validation and integrity checks
26,"Calculate statistical indicators and metrics: Develop functions to compute key performance indicators and statistical summaries for ED operations. Calculate: (1) Conversion rates - percentage of patients by urgency level transitioning to admission vs discharge. (2) Average wait times stratified by urgency level, time of day, day of week with confidence intervals. (3) Median and 90th percentile wait times for benchmarking. (4) Patient throughput - average patients per hour/day/shift. (5) Staff utilization - patients per staff member ratios. (6) Readmission rates - percentage of patients returning within 7/14/30 days. (7) Left without being seen (LWBS) rate if applicable. (8) Door-to-doctor time statistics. (9) Temporal aggregations showing trends. Create summary report functions generating formatted tables with these indicators. Implement confidence interval calculations using bootstrap or t-distribution methods. Visualize trends over time. Document interpretation guidelines for each metric. Deliverables: statistical_indicators.py module, indicators_report.md, summary_tables.csv, kpi_dashboard concept.",Suk Jin,Iter4 Week 2,Iter4 Week 3,,,,Conversion rates averages automatic calculations
27,"Model evaluation and performance assessment: Conduct comprehensive evaluation of all trained statistical models using rigorous methodology. For classification models: (1) Perform k-fold cross-validation (k=5 or 10) reporting mean and std dev of accuracy, precision, recall, F1 for each urgency class. (2) Create confusion matrices for test set showing misclassification patterns. (3) Generate ROC curves using one-vs-rest approach with AUC scores. (4) Calculate sensitivity and specificity especially for high-urgency cases (ESI 1-2) where accurate detection is critical. (5) Analyze class imbalance effects. For regression models: (1) Report R², adjusted R², RMSE, MAE on test set. (2) Create actual vs predicted scatter plots with diagonal reference line. (3) Analyze residual distributions. For Poisson model: (1) Goodness-of-fit statistics. (2) Deviance analysis. (3) Overdispersion assessment. Compare models within each category selecting best performer. Document model selection criteria. Discuss practical implications of model performance for ED operations. Deliverables: model_evaluation.ipynb comprehensive notebook (All), performance_metrics_summary.csv (All), model_selection_justification.md (All), ROC_curves.png (All).",All,Iter4 Week 2,Iter4 Week 3,,,,Cross-validation and performance metrics
27.5,"PySpark large-scale data analysis (OPTIONAL - advanced analysis): Load ED data into PySpark for distributed processing and advanced analytics. Create SparkSession and load SQLite data into PySpark DataFrames using appropriate schema. Perform data cleaning and preprocessing in PySpark: check null values, validate data ranges, remove duplicates, standardize formats. Create temporary SQL views for analysis. Conduct analyses similar to InClassWork_05: (1) Average wait times by city, urgency level, and vehicle type using GROUP BY and aggregations. (2) Staff members with highest on-time performance rates. (3) Correlation analysis between vital signs and wait times using CORR function. (4) Outlier detection for wait times and vital signs using IQR method with PERCENTILE_APPROX. (5) Aggregate patient satisfaction metrics by staff and department using window functions (ROW_NUMBER, PARTITION BY). Use PySpark SQL for complex queries with CTEs and window functions. Compare PySpark performance with Pandas for large datasets. Document when to use PySpark vs Pandas. Create visualizations of PySpark analysis results. This demonstrates big data processing skills and scalability considerations. Deliverables: pyspark_analysis.ipynb notebook with SparkSession creation (Suk Jin or Shaobo), pyspark_results.md documenting findings (Suk Jin or Shaobo), performance_comparison.md comparing PySpark vs Pandas (Suk Jin or Shaobo).",Suk Jin or Shaobo,Iter4 Week 2,Iter4 Week 3,,,,Optional PySpark distributed analysis
28,"Create model visualization and interpretation plots: Develop visualizations to interpret and communicate statistical model results for non-technical stakeholders. For classification models: (1) Feature importance plots showing which variables most influence urgency prediction - use coefficient magnitudes for logistic regression, discriminant function coefficients for LDA. (2) Confusion matrix heatmaps with annotations. (3) ROC curves with AUC scores. (4) Probability calibration plots. For regression models: (1) Coefficient plots with confidence intervals showing effect sizes. (2) Partial dependence plots showing relationship between individual features and predicted wait time. (3) Residual plots (residuals vs fitted, QQ plot, scale-location, residuals vs leverage). (4) Actual vs predicted scatter plots with trend line. For Poisson model: (1) Predicted vs observed count plots. (2) Effect plots showing how different factors influence daily visit counts. Create executive summary dashboard combining key visualizations. Use clear labels, legends, and annotations. Export publication-quality figures (300 DPI, appropriate sizing). Deliverables: model_visualizations.ipynb notebook, figures/ directory with all plots as PNG/PDF, executive_dashboard mockup.",Xiaobai,Iter4 Week 2,Iter4 Week 3,,,,Feature importance and prediction plots
29,"Design Flask API endpoints: Design and implement RESTful API using Flask framework to serve as integration layer between frontend and database. Create API endpoints: (1) GET /api/patients - retrieve patient list with filtering parameters. (2) GET /api/patients/<id> - retrieve specific patient details. (3) GET /api/visits - retrieve visit records with date range filters. (4) GET /api/analytics/wait-times - execute SQL query returning wait time statistics by urgency level. (5) GET /api/analytics/volume - return patient volume trends. (6) POST /api/predict/urgency - accept patient vital signs and return predicted urgency level using trained classification model. (7) POST /api/predict/wait-time - return predicted wait time using trained regression model. Implement request validation, error handling with appropriate HTTP status codes, JSON response formatting. Use Flask-CORS for cross-origin requests. Document API using Swagger/OpenAPI specification. Implement rate limiting. Deliverables: app.py Flask application, api_documentation.md, requirements.txt, Postman collection for testing.",Suk Jin,Iter4 Week 1,Iter4 Week 2,,,,Flask API design receive frontend requests return JSON responses
29.5,"Implement database access layer: Create data access layer separating database operations from API endpoint logic following repository pattern. Develop db_manager.py module with classes: (1) PatientRepository with methods for CRUD operations on patients table. (2) VisitRepository for visit records. (3) AnalyticsRepository with methods executing complex analytical SQL queries returning results as JSON-serializable dictionaries. Use SQLAlchemy ORM for database interactions providing abstraction over raw SQL. Implement connection pooling for performance. Add query logging for debugging. Implement parameterized queries preventing SQL injection. Handle database exceptions gracefully returning informative error messages to API layer. Write integration tests verifying database operations. Document data access patterns. This ensures frontend NEVER directly accesses database - all database operations go through backend API calling this data access layer. Deliverables: db_manager.py module, db_config.py configuration, test_db_access.py integration tests, data_access_documentation.md.",Suk Jin,Iter4 Week 2,Iter4 Week 3,,,,Backend executes SQL queries on behalf of frontend
30,"Build Flask frontend web interface: Develop web-based user interface using Flask templates (Jinja2), HTML, CSS, and JavaScript to provide interactive ED data exploration. Create pages: (1) Dashboard page displaying key metrics (current patient count, average wait times, staff on duty) with auto-refresh. (2) Patient list page with search/filter capabilities calling GET /api/patients endpoint. (3) Analytics page with dropdown filters (date range, urgency level) calling analytics endpoints and displaying charts using Chart.js or Plotly.js. (4) Prediction form page with input fields for vital signs calling POST /api/predict endpoints and displaying predicted urgency and wait time. (5) Visualization gallery displaying pre-generated statistical plots. Use Bootstrap for responsive design. Implement client-side form validation. Add loading indicators for API calls. Handle API errors gracefully with user-friendly messages. Ensure accessibility standards. IMPORTANT CONSTRAINT: Frontend must call backend API endpoints for ALL data - no direct database connections allowed. Test API integration thoroughly. Deliverables: templates/ directory with HTML files, static/ directory with CSS/JS, frontend_documentation.md, screenshots of interface.",Xiaobai,Iter4 Week 1,Iter4 Week 3,,,,Web UI with forms calling backend API and chart display
31,"Compile final report content: Collaboratively write comprehensive final project report documenting entire project lifecycle and results. Report structure: (1) Introduction section presenting ED overcrowding problem, project motivation, specific research questions (urgency prediction, wait time modeling, volume forecasting), and objectives. (2) Background/Literature Review briefly covering ED triage systems, Emergency Severity Index, relevant statistical methods. (3) Methods section with three subsections: (a) Database Design - ERD diagram, schema description, normalization approach, data generation methodology citing sources (CDC NHAMCS, data.gov distributions), ETL pipeline overview. (b) Data Analysis - exploratory analysis findings, data cleaning steps, descriptive statistics, correlation analysis. (c) Statistical Modeling - detailed description of classification models (logistic, LDA, Naive Bayes), regression models (linear regression), and Poisson regression with mathematical formulations, feature engineering, train/test split strategy, hyperparameter tuning approach. (4) Results section presenting model performance metrics, confusion matrices, coefficient interpretations, diagnostic plots, key findings from SQL analytical queries, statistical indicators computed. (5) Discussion section interpreting results in ED operational context, practical implications for resource allocation and triage optimization, model limitations, assumptions, potential biases. (6) Conclusions summarizing key contributions and findings. (7) Future Work suggesting extensions (additional models, real-time prediction system, expanded features). (8) References citing course materials, textbooks, data sources, technical documentation. Aim for 10-15 pages. Deliverables: final_report_draft.md content file with all sections, references.bib if using BibTeX.",All,Iter5 Week 1,Iter5 Week 1,,,,Methods results recommendations
32,"Format final report with figures in Overleaf: Transfer compiled report content to Overleaf LaTeX document with professional formatting. Create LaTeX document structure with appropriate document class (article or report), packages (graphicx for figures, booktabs for tables, amsmath for equations, hyperref for links, listings for code). Design title page with project title, team members with NUIDs, course information, date. Format sections with hierarchical numbering. Embed all figures with captions and labels: ERD diagram, confusion matrices, ROC curves, residual plots, time series visualizations, correlation heatmaps. Create professional tables using booktabs: model performance metrics, summary statistics, coefficient tables. Format equations properly using math mode: regression equations, logistic regression formula, Poisson GLM specification. Include code listings if relevant with syntax highlighting. Ensure consistent formatting: font sizes, spacing, margins. Generate bibliography from references. Proofread for LaTeX compilation errors, broken references, misaligned figures. Export final PDF. Aim for polished, publication-quality formatting. Deliverables: final_report.tex LaTeX source, final_report.pdf compiled document, figures/ directory with all images in appropriate format (PNG/PDF), references.bib bibliography file.",Xiaobai,Iter5 Week 1,Iter5 Week 2,,,,Overleaf LaTeX formatting
33,"Create presentation slides for class: Develop compelling presentation (15-20 slides) for class delivery. Slide outline: (1) Title slide with project name, team, course. (2) Motivation slide presenting ED overcrowding problem with statistics. (3) Research questions slide listing three specific questions addressed. (4) Database design slide showing ERD diagram and schema overview. (5) Data generation slide explaining simulated data approach and distributions used. (6) Exploratory analysis slides (2-3) showing key visualizations (patient volume trends, wait time distributions, urgency level breakdown). (7) Modeling approach slide overview listing models used. (8) Classification results slide with confusion matrix and accuracy metrics. (9) Regression results slide with R² and coefficient interpretation. (10) Poisson regression slide with predicted visit counts. (11) Key findings slides (2-3) highlighting actionable insights for ED operations. (12) Optional web application demo slide if built. (13) Challenges and limitations slide. (14) Conclusions slide summarizing contributions. (15) Future work slide. (16) Questions slide. Use visual design principles: minimal text, clear visualizations, consistent color scheme, readable fonts. Include speaker notes. Practice timing for 10-15 minute presentation. Deliverables: presentation.pptx or Google Slides, presenter_notes.md, demo screenshots if applicable.",Xiaobai,Iter5 Week 1,Iter5 Week 2,,,,Class presentation materials
34,"Finalize GitHub repository for submission: Prepare GitHub repository for final evaluation ensuring professional organization and comprehensive documentation. Repository structure: (1) Root directory with README.md comprehensive project overview, requirements.txt for dependencies, .gitignore configured. (2) data/ directory with sample generated datasets (or data generation scripts if datasets large), data dictionary. (3) database/ directory with schema.sql, ERD diagram, ETL scripts. (4) notebooks/ directory with Jupyter notebooks for EDA, modeling, visualizations. (5) scripts/ directory with Python modules (data_generation.py, business_logic.py, statistical_indicators.py, model training scripts). (6) models/ directory with trained model files (.pkl) and evaluation results. (7) sql/ directory with analytical_queries.sql. (8) reports/ directory with LaTeX source and final PDF report. (9) presentation/ directory with slides. (10) web/ directory if Flask app built with separate backend/ and frontend/ subdirectories. Update README.md with: project description, team roles, setup instructions, usage guide, file structure explanation, key results summary, links to report and presentation. Ensure all code documented with docstrings. Check all file paths work. Remove sensitive information. Add LICENSE file. Tag final release version. Test repository by cloning fresh and following setup instructions. Deliverables: organized repository, comprehensive README.md, all code and documents committed, release tag created.",Suk Jin,Iter5 Week 2,Iter5 Week 2,,,,README documentation code organization
35,"Practice presentation delivery: Conduct team rehearsal of class presentation to ensure smooth delivery and proper timing. Practice activities: (1) Each team member practices their assigned sections (Shaobo: database/data generation, Suk Jin: modeling approach and results, Xiaobai: visualizations and conclusions). (2) Ensure smooth transitions between speakers. (3) Time presentation to stay within allocated time limit (likely 10-15 minutes) with buffer for questions. (4) Practice demo if web application built - test on different machine to verify portability, prepare backup plan if live demo fails (use screenshots/video recording). (5) Anticipate potential questions and prepare responses (Why simulated data? Why these specific models? How would this work with real data? What are limitations?). (6) Get feedback from each other on clarity, pace, engagement. (7) Refine slides based on practice session feedback. (8) Prepare equipment: test laptop connections, backup presentation on multiple devices, ensure visualizations render properly. Conduct at least 2 full rehearsals. Deliverables: timing notes, Q&A preparation document, refined presentation slides.",All,Iter5 Week 2,Iter5 Week 2,,,,Rehearse delivery and timing
36,"Submit final deliverables: Package and submit all final project deliverables according to course requirements via Canvas LMS. Submission checklist: (1) Final project report PDF (10-15 pages) uploaded to Canvas. (2) Presentation slides (PPTX/PDF) uploaded to Canvas. (3) GitHub repository link submitted - verify repository is public and accessible. (4) Updated progress tracker CSV showing all tasks with actual completion dates and status. (5) Optional: web application deployment link if hosted online, or detailed local setup instructions. (6) Any supplementary materials requested by instructor. Verify before submitting: (1) All file names follow naming convention specified. (2) PDFs are not corrupted and open correctly. (3) GitHub link works when opened in incognito/private browser. (4) All team member names and NUIDs included in report. (5) Report includes all required sections. (6) No placeholder text or TODO comments remaining. Submit before deadline with buffer time for unexpected issues. Confirm submission successful by checking Canvas confirmation. Deliverables: Canvas submission confirmation, all required files uploaded, GitHub repository link verified accessible.",Suk Jin,Iter5 Week 2,Iter5 Week 2,,,,Report slides code submission
37,"Present to class: Deliver final project presentation to DS5110 class demonstrating completed work and findings. Presentation execution: (1) Arrive early to set up equipment and test connections. (2) Introduction by team spokesperson presenting team and project overview (1 min). (3) Shaobo presents database design, schema, and data generation approach (3-4 min). (4) Suk Jin presents statistical modeling methodology, results from classification/regression/Poisson models with key performance metrics and interpretations (4-5 min). (5) Xiaobai presents visualizations, insights from exploratory analysis, actionable recommendations for ED operations (3-4 min). (6) Optional live demo of web application if built showing data exploration and prediction features (2 min). (7) Conclusions summarizing project contributions and lessons learned (1 min). (8) Q&A session responding to instructor and classmate questions professionally and thoroughly (3-5 min). Maintain engagement: make eye contact, speak clearly, use pointer to highlight key elements on slides, show enthusiasm for work. Handle technical difficulties calmly with backup plan. Stay within time limit. Thank audience at conclusion. Deliverables: successful presentation delivery, audience questions answered, evaluation feedback received from instructor.",All,Iter5 Week 2,Iter5 Week 2,,,,Class presentation and demo
